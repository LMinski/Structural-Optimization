{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTT = loadmat(f\"Data04-08_TT_UltVag_Cut.mat\")\n",
    "\n",
    "dataVG = loadmat(f\"Data04-08_TF_UltVag_Cut.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataBaseline = dataTT['Baseline']\n",
    "dataCincoP = dataTT['CincoP']\n",
    "dataDez = dataTT['DezP']\n",
    "dataVinte = dataTT['VinteP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame()\n",
    "\n",
    "X_train = pd.DataFrame(dataBaseline)\n",
    "\n",
    "\n",
    "dataCincoP_df = pd.DataFrame(dataCincoP)\n",
    "X_test = pd.concat([X_test, dataCincoP_df], ignore_index=True)\n",
    "\n",
    "dataDez_df = pd.DataFrame(dataDez)\n",
    "X_test = pd.concat([X_test, dataDez_df], ignore_index=True)\n",
    "\n",
    "dataVinte_df = pd.DataFrame(dataVinte)\n",
    "X_test = pd.concat([X_test, dataVinte_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O modelo tem que ser treinado apenas com os dados baseline? \n",
    "#Ver como monta o autoencoder\n",
    "#Ver como representar a regressão a partir do encoder e pegar o MSE/MAE em relação ao dado analisado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(0)\n",
    "X_test = X_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>5820</th>\n",
       "      <th>5821</th>\n",
       "      <th>5822</th>\n",
       "      <th>5823</th>\n",
       "      <th>5824</th>\n",
       "      <th>5825</th>\n",
       "      <th>5826</th>\n",
       "      <th>5827</th>\n",
       "      <th>5828</th>\n",
       "      <th>5829</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.332179</td>\n",
       "      <td>0.322156</td>\n",
       "      <td>0.312134</td>\n",
       "      <td>0.324365</td>\n",
       "      <td>0.337422</td>\n",
       "      <td>0.335790</td>\n",
       "      <td>0.325244</td>\n",
       "      <td>0.313052</td>\n",
       "      <td>0.296634</td>\n",
       "      <td>0.280217</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404234</td>\n",
       "      <td>-0.420915</td>\n",
       "      <td>-0.427830</td>\n",
       "      <td>-0.423002</td>\n",
       "      <td>-0.420707</td>\n",
       "      <td>-0.438448</td>\n",
       "      <td>-0.456189</td>\n",
       "      <td>-0.459560</td>\n",
       "      <td>-0.458648</td>\n",
       "      <td>-0.457411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.386385</td>\n",
       "      <td>0.379305</td>\n",
       "      <td>0.364307</td>\n",
       "      <td>0.348443</td>\n",
       "      <td>0.340129</td>\n",
       "      <td>0.352088</td>\n",
       "      <td>0.364048</td>\n",
       "      <td>0.359345</td>\n",
       "      <td>0.345326</td>\n",
       "      <td>0.331428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.380740</td>\n",
       "      <td>-0.384124</td>\n",
       "      <td>-0.395138</td>\n",
       "      <td>-0.407912</td>\n",
       "      <td>-0.420737</td>\n",
       "      <td>-0.433787</td>\n",
       "      <td>-0.446837</td>\n",
       "      <td>-0.460072</td>\n",
       "      <td>-0.473456</td>\n",
       "      <td>-0.486840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196733</td>\n",
       "      <td>0.204847</td>\n",
       "      <td>0.193909</td>\n",
       "      <td>0.176075</td>\n",
       "      <td>0.176909</td>\n",
       "      <td>0.173954</td>\n",
       "      <td>0.161669</td>\n",
       "      <td>0.151720</td>\n",
       "      <td>0.143542</td>\n",
       "      <td>0.142185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264166</td>\n",
       "      <td>-0.266663</td>\n",
       "      <td>-0.277983</td>\n",
       "      <td>-0.288434</td>\n",
       "      <td>-0.295890</td>\n",
       "      <td>-0.290828</td>\n",
       "      <td>-0.275072</td>\n",
       "      <td>-0.281777</td>\n",
       "      <td>-0.292735</td>\n",
       "      <td>-0.297212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.321910</td>\n",
       "      <td>0.323861</td>\n",
       "      <td>0.331860</td>\n",
       "      <td>0.335426</td>\n",
       "      <td>0.324528</td>\n",
       "      <td>0.313631</td>\n",
       "      <td>0.304671</td>\n",
       "      <td>0.296135</td>\n",
       "      <td>0.295391</td>\n",
       "      <td>0.306014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.491216</td>\n",
       "      <td>-0.496316</td>\n",
       "      <td>-0.505899</td>\n",
       "      <td>-0.513626</td>\n",
       "      <td>-0.514413</td>\n",
       "      <td>-0.515201</td>\n",
       "      <td>-0.507789</td>\n",
       "      <td>-0.498291</td>\n",
       "      <td>-0.501124</td>\n",
       "      <td>-0.523807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.303952</td>\n",
       "      <td>0.298020</td>\n",
       "      <td>0.303201</td>\n",
       "      <td>0.309421</td>\n",
       "      <td>0.319088</td>\n",
       "      <td>0.328755</td>\n",
       "      <td>0.308712</td>\n",
       "      <td>0.286715</td>\n",
       "      <td>0.285506</td>\n",
       "      <td>0.295734</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.449191</td>\n",
       "      <td>-0.442378</td>\n",
       "      <td>-0.445943</td>\n",
       "      <td>-0.449280</td>\n",
       "      <td>-0.450386</td>\n",
       "      <td>-0.451493</td>\n",
       "      <td>-0.447657</td>\n",
       "      <td>-0.442582</td>\n",
       "      <td>-0.439880</td>\n",
       "      <td>-0.439493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.352563</td>\n",
       "      <td>0.350199</td>\n",
       "      <td>0.347836</td>\n",
       "      <td>0.350021</td>\n",
       "      <td>0.357670</td>\n",
       "      <td>0.365319</td>\n",
       "      <td>0.351699</td>\n",
       "      <td>0.335612</td>\n",
       "      <td>0.323804</td>\n",
       "      <td>0.320377</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.414459</td>\n",
       "      <td>-0.426089</td>\n",
       "      <td>-0.437719</td>\n",
       "      <td>-0.436799</td>\n",
       "      <td>-0.430328</td>\n",
       "      <td>-0.428078</td>\n",
       "      <td>-0.452838</td>\n",
       "      <td>-0.477598</td>\n",
       "      <td>-0.503919</td>\n",
       "      <td>-0.531385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.307783</td>\n",
       "      <td>0.309593</td>\n",
       "      <td>0.312903</td>\n",
       "      <td>0.312240</td>\n",
       "      <td>0.303620</td>\n",
       "      <td>0.295548</td>\n",
       "      <td>0.294303</td>\n",
       "      <td>0.293058</td>\n",
       "      <td>0.287178</td>\n",
       "      <td>0.280250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.349849</td>\n",
       "      <td>-0.343723</td>\n",
       "      <td>-0.358625</td>\n",
       "      <td>-0.385524</td>\n",
       "      <td>-0.400667</td>\n",
       "      <td>-0.396465</td>\n",
       "      <td>-0.392827</td>\n",
       "      <td>-0.393367</td>\n",
       "      <td>-0.393906</td>\n",
       "      <td>-0.397513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.306879</td>\n",
       "      <td>0.304073</td>\n",
       "      <td>0.300413</td>\n",
       "      <td>0.295898</td>\n",
       "      <td>0.290179</td>\n",
       "      <td>0.287073</td>\n",
       "      <td>0.291591</td>\n",
       "      <td>0.295498</td>\n",
       "      <td>0.293633</td>\n",
       "      <td>0.291768</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.321043</td>\n",
       "      <td>-0.316996</td>\n",
       "      <td>-0.310592</td>\n",
       "      <td>-0.310783</td>\n",
       "      <td>-0.315410</td>\n",
       "      <td>-0.326124</td>\n",
       "      <td>-0.344636</td>\n",
       "      <td>-0.359716</td>\n",
       "      <td>-0.365918</td>\n",
       "      <td>-0.371230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.330445</td>\n",
       "      <td>0.337847</td>\n",
       "      <td>0.341394</td>\n",
       "      <td>0.339669</td>\n",
       "      <td>0.337945</td>\n",
       "      <td>0.340572</td>\n",
       "      <td>0.343458</td>\n",
       "      <td>0.344882</td>\n",
       "      <td>0.344627</td>\n",
       "      <td>0.344371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.524975</td>\n",
       "      <td>-0.524026</td>\n",
       "      <td>-0.529381</td>\n",
       "      <td>-0.534737</td>\n",
       "      <td>-0.534210</td>\n",
       "      <td>-0.530939</td>\n",
       "      <td>-0.528307</td>\n",
       "      <td>-0.528174</td>\n",
       "      <td>-0.528040</td>\n",
       "      <td>-0.546352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.268962</td>\n",
       "      <td>0.264050</td>\n",
       "      <td>0.258670</td>\n",
       "      <td>0.250289</td>\n",
       "      <td>0.240952</td>\n",
       "      <td>0.241666</td>\n",
       "      <td>0.245034</td>\n",
       "      <td>0.236039</td>\n",
       "      <td>0.224396</td>\n",
       "      <td>0.222093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343877</td>\n",
       "      <td>-0.341053</td>\n",
       "      <td>-0.333657</td>\n",
       "      <td>-0.338608</td>\n",
       "      <td>-0.344879</td>\n",
       "      <td>-0.347073</td>\n",
       "      <td>-0.348988</td>\n",
       "      <td>-0.372113</td>\n",
       "      <td>-0.395885</td>\n",
       "      <td>-0.373837</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 5830 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.332179  0.322156  0.312134  0.324365  0.337422  0.335790  0.325244   \n",
       "1    0.386385  0.379305  0.364307  0.348443  0.340129  0.352088  0.364048   \n",
       "2    0.196733  0.204847  0.193909  0.176075  0.176909  0.173954  0.161669   \n",
       "3    0.321910  0.323861  0.331860  0.335426  0.324528  0.313631  0.304671   \n",
       "4    0.303952  0.298020  0.303201  0.309421  0.319088  0.328755  0.308712   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.352563  0.350199  0.347836  0.350021  0.357670  0.365319  0.351699   \n",
       "996  0.307783  0.309593  0.312903  0.312240  0.303620  0.295548  0.294303   \n",
       "997  0.306879  0.304073  0.300413  0.295898  0.290179  0.287073  0.291591   \n",
       "998  0.330445  0.337847  0.341394  0.339669  0.337945  0.340572  0.343458   \n",
       "999  0.268962  0.264050  0.258670  0.250289  0.240952  0.241666  0.245034   \n",
       "\n",
       "         7         8         9     ...      5820      5821      5822  \\\n",
       "0    0.313052  0.296634  0.280217  ... -0.404234 -0.420915 -0.427830   \n",
       "1    0.359345  0.345326  0.331428  ... -0.380740 -0.384124 -0.395138   \n",
       "2    0.151720  0.143542  0.142185  ... -0.264166 -0.266663 -0.277983   \n",
       "3    0.296135  0.295391  0.306014  ... -0.491216 -0.496316 -0.505899   \n",
       "4    0.286715  0.285506  0.295734  ... -0.449191 -0.442378 -0.445943   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.335612  0.323804  0.320377  ... -0.414459 -0.426089 -0.437719   \n",
       "996  0.293058  0.287178  0.280250  ... -0.349849 -0.343723 -0.358625   \n",
       "997  0.295498  0.293633  0.291768  ... -0.321043 -0.316996 -0.310592   \n",
       "998  0.344882  0.344627  0.344371  ... -0.524975 -0.524026 -0.529381   \n",
       "999  0.236039  0.224396  0.222093  ... -0.343877 -0.341053 -0.333657   \n",
       "\n",
       "         5823      5824      5825      5826      5827      5828      5829  \n",
       "0   -0.423002 -0.420707 -0.438448 -0.456189 -0.459560 -0.458648 -0.457411  \n",
       "1   -0.407912 -0.420737 -0.433787 -0.446837 -0.460072 -0.473456 -0.486840  \n",
       "2   -0.288434 -0.295890 -0.290828 -0.275072 -0.281777 -0.292735 -0.297212  \n",
       "3   -0.513626 -0.514413 -0.515201 -0.507789 -0.498291 -0.501124 -0.523807  \n",
       "4   -0.449280 -0.450386 -0.451493 -0.447657 -0.442582 -0.439880 -0.439493  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995 -0.436799 -0.430328 -0.428078 -0.452838 -0.477598 -0.503919 -0.531385  \n",
       "996 -0.385524 -0.400667 -0.396465 -0.392827 -0.393367 -0.393906 -0.397513  \n",
       "997 -0.310783 -0.315410 -0.326124 -0.344636 -0.359716 -0.365918 -0.371230  \n",
       "998 -0.534737 -0.534210 -0.530939 -0.528307 -0.528174 -0.528040 -0.546352  \n",
       "999 -0.338608 -0.344879 -0.347073 -0.348988 -0.372113 -0.395885 -0.373837  \n",
       "\n",
       "[1000 rows x 5830 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m initial_params \u001b[39m=\u001b[39m [\u001b[39m0.1\u001b[39m, \u001b[39m0.05\u001b[39m, \u001b[39m3.0\u001b[39m, \u001b[39m1e-5\u001b[39m, \u001b[39m0.001\u001b[39m, \u001b[39m0.1\u001b[39m]\n\u001b[1;32m     45\u001b[0m \u001b[39m# Otimização dos hiperparâmetros\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m result \u001b[39m=\u001b[39m minimize(cost_function, initial_params, args\u001b[39m=\u001b[39;49m(X_train,), method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m'\u001b[39;49m, bounds\u001b[39m=\u001b[39;49m[(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), (\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), (\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), (\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), (\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), (\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)])\n\u001b[1;32m     48\u001b[0m \u001b[39m# Exibir os resultados otimizados\u001b[39;00m\n\u001b[1;32m     49\u001b[0m optimal_params \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mx\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    711\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    712\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:361\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    355\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    362\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    363\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:286\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m--> 286\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_grad\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated:\n\u001b[0;32m--> 256\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad_impl()\n\u001b[1;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m approx_derivative(fun_wrapped, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, f0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf,\n\u001b[1;32m    174\u001b[0m                            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfinite_diff_options)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m sparsity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[1;32m    506\u001b[0m                              use_one_sided, method)\n\u001b[1;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(sparsity) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sparsity) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[39m=\u001b[39m x[i] \u001b[39m-\u001b[39m x0[i]  \u001b[39m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[39m=\u001b[39m fun(x) \u001b[39m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m3-point\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_1d(fun(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`fun` return value has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mmore than 1 dimension.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[42], line 28\u001b[0m, in \u001b[0;36mcost_function\u001b[0;34m(params, X_train)\u001b[0m\n\u001b[1;32m     26\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mlegacy\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mepsilon)\n\u001b[1;32m     27\u001b[0m autoencoder\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 28\u001b[0m autoencoder\u001b[39m.\u001b[39;49mfit(X_train, X_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39m# Calcular a variância do loss\u001b[39;00m\n\u001b[1;32m     31\u001b[0m losses \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Função de custo para otimização\n",
    "def cost_function(params, X_train):\n",
    "    sparsity, rho, beta, epsilon, l1_lambda, weight_variance = params\n",
    "\n",
    "    # Treinamento do modelo\n",
    "    num_terms = X_train.columns.shape[0]\n",
    "    input_layer = Input(shape=(num_terms,))\n",
    "    encoded = Dense(128, activation='relu')(input_layer)\n",
    "    encoded = Dense(64, activation='relu')(encoded)\n",
    "    encoded = Dense(32, activation='relu')(encoded)\n",
    "    encoded_sparse = Dense(16, activation='relu', activity_regularizer=tf.keras.regularizers.l1(l1_lambda))(encoded)\n",
    "    decoded = Dense(32, activation='relu')(encoded_sparse)\n",
    "    decoded = Dense(64, activation='relu')(decoded)\n",
    "    decoded = Dense(128, activation='relu')(decoded)\n",
    "    decoded = Dense(num_terms, activation='sigmoid')(decoded)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "\n",
    "    activity_loss = beta * tf.keras.backend.sum(tf.keras.backend.square(rho - tf.keras.backend.mean(encoded_sparse, axis=0)))\n",
    "    autoencoder.add_loss(activity_loss)\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=epsilon)\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "    autoencoder.fit(X_train, X_train, epochs=50, batch_size=64, verbose=0)\n",
    "\n",
    "    # Calcular a variância do loss\n",
    "    losses = autoencoder.history.history[\"loss\"]\n",
    "    loss_variance = tf.math.reduce_variance(losses).numpy()\n",
    "\n",
    "    # Ponderação entre a variância e o valor do loss\n",
    "    weighted_loss = autoencoder.history.history[\"loss\"][-1] + weight_variance * loss_variance\n",
    "\n",
    "    return weighted_loss\n",
    "\n",
    "# Seus dados de treinamento\n",
    "# Substitua X_train pelos seus dados de treinamento\n",
    "\n",
    "# Valores iniciais dos hiperparâmetros\n",
    "initial_params = [0.1, 0.05, 3.0, 1e-5, 0.001, 0.1]\n",
    "\n",
    "# Otimização dos hiperparâmetros\n",
    "result = minimize(cost_function, initial_params, args=(X_train,), method='L-BFGS-B', bounds=[(0, 1), (0, 1), (0, None), (0, None), (0, None), (0, None)])\n",
    "\n",
    "# Exibir os resultados otimizados\n",
    "optimal_params = result.x\n",
    "optimal_cost = result.fun\n",
    "\n",
    "print(\"Hiperparâmetros otimizados:\", optimal_params)\n",
    "print(\"Custo otimizado:\", optimal_cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 19ms/step - loss: 5.3208\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.8848\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.3343\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.2351\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.3488\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.6141\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.9608\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.4288\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 14.8826\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 21.6215\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 32.9313\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 52.6773\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 73.5364\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 96.8046\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 127.8490\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 155.8484\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 177.8810\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 211.1293\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 258.5192\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 320.4710\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 394.6298\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 479.5480\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 575.4159\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 682.7092\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 801.1542\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 933.3704\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1079.3113\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1239.9838\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1415.0483\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1604.9473\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1811.1390\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2032.1904\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2271.2976\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2525.8850\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2794.0190\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3073.7217\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 3360.0020\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3650.0796\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3945.1511\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4242.4766\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4541.3330\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4843.0659\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5140.3677\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5432.7085\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5715.9971\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5991.6958\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6252.7798\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6498.9263\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6732.2949\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6947.9209\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 18ms/step - loss: 8.2876\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.9408\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.1568\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.1119\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.3603\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 12.7989\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.6175\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 21.5573\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 28.5315\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 37.7912\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 49.6480\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 60.2204\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 72.0050\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 87.9754\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 103.6189\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 122.3688\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 142.7313\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 164.7938\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 185.5022\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 208.7965\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 225.8515\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 246.2933\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 263.4992\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 280.1993\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 293.7887\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 306.9595\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 312.9648\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 320.5502\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 325.0413\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 326.4264\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 329.4410\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 325.7742\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 326.0607\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 324.9978\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 322.8082\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 320.9810\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 319.4294\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 314.2129\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 315.2419\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 311.9079\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 310.6254\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 309.3450\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 304.3428\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 301.9743\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 293.7066\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 290.3494\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 288.2561\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 288.3080\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 290.6391\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 294.4507\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 19ms/step - loss: 8.8986\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.9508\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4344\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.5661\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 4.6487\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.7010\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.5781\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.3729\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4442\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4394\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.3167\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.4432\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.7296\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.8763\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.0999\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.5453\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.8481\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.1572\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.5751\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.9746\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.8849\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.9465\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.8570\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.2271\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.7340\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.5384\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.4392\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.3580\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 11.0928\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.0094\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.3445\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.0291\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.9295\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.3096\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.1125\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.2229\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.7317\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.1933\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.4735\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.5715\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.0592\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.5145\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.6722\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.7481\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.8194\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 13.0198\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 13.6384\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 13.2903\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 13.6282\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 13.3498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2418.573803583781"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoderEsparso(0.1, 0.05, 3.0, 1e-5, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 4.8815\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 4.0343\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.6618\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.9954\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.6695\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.8398\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.8402\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 10.4094\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 11.2645\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 13.6940\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.8703\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 19.0612\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 22.4006\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 29.0247\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 35.4738\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 37.2417\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 41.1153\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 47.0868\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 54.6709\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 62.2067\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 69.8292\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 77.3995\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 84.8515\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 92.0201\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 98.6706\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 104.2885\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 109.0659\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 112.8271\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 115.3894\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 117.6432\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 118.7902\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 119.5175\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 119.9664\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.0688\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.0980\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.0768\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.1591\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 120.3397\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.4338\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.5410\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.3226\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.4122\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.3712\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.1934\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.5848\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.6959\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 120.6219\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 120.7409\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 120.7196\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 121.1047\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 5.1488\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.2288\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2.1317\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.6571\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1271\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.8706\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.6252\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.7367\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 0.9201\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.0342\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0538\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.0280\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.1218\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1.2549\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.3478\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1.3649\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 1.6227\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 3.1166\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 5.5221\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 7.7223\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.0934\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.5537\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.6843\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 13.1409\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 14.5582\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.2680\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 18.1323\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 20.9855\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 26.0472\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 30.5492\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 13ms/step - loss: 38.4006\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 47.3861\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 58.8814\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 73.7866\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 93.4237\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 118.3622\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 145.0382\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 170.8445\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 194.8853\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 219.6821\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 243.7323\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 268.8194\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 293.8177\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 317.2085\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 340.1879\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 364.4214\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 388.7820\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 414.3418\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 442.2564\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 472.7865\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 12ms/step - loss: 8.5984\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.8724\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.0585\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.3193\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4.9330\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.9924\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.6901\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 8.9765\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.8311\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.5727\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.0669\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.6989\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.9646\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 6.7543\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.0423\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.7005\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.9388\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.9280\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.1263\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.2007\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.2340\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.6155\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.1759\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.6648\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.6978\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.4437\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.7019\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 9.9098\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.2378\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.2666\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.3177\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.4856\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.3969\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.5044\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.5773\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.5170\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.5890\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.7096\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.8053\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.0960\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.7935\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.7236\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.6888\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.7767\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.8501\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.0418\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.0838\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.1019\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 11.1248\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.1001\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 15ms/step - loss: 8.5334\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.6824\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 15ms/step - loss: 7.7499\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.4172\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 8.3175\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 11.6612\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 19.1808\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 29.1981\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 42.1756\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 57.1718\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 71.2695\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 82.9603\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 94.7097\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 104.9326\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 115.1541\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 125.1186\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 134.7129\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 143.2635\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 152.9073\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 165.3000\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 180.0875\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 192.3518\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 203.8770\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 217.6657\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 235.1834\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 254.7836\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 277.0595\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 302.5024\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 330.6482\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 362.9460\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 398.3591\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 437.4722\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 479.6137\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 525.1900\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 573.1348\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 626.0558\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 681.9002\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 740.0150\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802.8538\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 868.8912\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 937.4257\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1011.1910\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1083.9083\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1159.1658\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1233.8975\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1308.5581\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1382.5530\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1456.7448\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1529.7043\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1603.7700\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 17ms/step - loss: 9.8603\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.3107\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.6912\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 11.3950\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 12.5007\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 14.4701\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 16.0357\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 17.7196\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 19.5156\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 21.8457\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 23.8912\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 25.1729\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 26.3063\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 28.0531\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 30.5366\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 32.9506\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 34.7696\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 36.3993\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 37.9256\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 39.6587\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 41.4864\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 43.3307\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 44.9618\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 46.0317\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 46.9585\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 47.5717\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 48.2167\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 48.5837\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 48.8802\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 48.7690\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 48.7149\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 48.5909\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 48.1745\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 47.6634\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 47.1980\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 46.5733\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 45.8777\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 45.1151\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 44.2931\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 43.3458\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 42.2136\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 41.0966\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 39.7247\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 38.3975\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 37.3869\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 37.0644\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 37.3594\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 37.9255\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 38.6415\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 39.0997\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 13ms/step - loss: 8.7416\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 6.8160\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.4239\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.6838\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.2294\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 7.9022\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.4276\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 8.6286\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.9549\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.3320\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.6594\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.9288\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.3496\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.5627\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 10.4679\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.4863\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.6416\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.0379\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.1579\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.0539\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.9435\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.1463\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4.6719\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.1965\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.5866\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.8089\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.6236\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 5.2365\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 5.6663\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 6.4353\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.1757\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 7.8052\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 8.4967\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 9.6095\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 10.7916\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 12.6809\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 17.2418\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 23.6415\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 32.5369\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 39.9527\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 45.5110\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 50.4539\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 55.4427\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 59.8980\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 63.9074\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 65.3730\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 67.7969\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 69.8620\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 72.4519\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 73.0170\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 796139797833347760128.0000\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1838673220129995620352.0000\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 448410009651514441728.0000\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 208245672713425780736.0000\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 937408499838472945664.0000\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1372567828670728634368.0000\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1483745940871810383872.0000\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1506746246381256572928.0000\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1511286719230576164864.0000\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512311569620779663360.0000\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512682694377572663296.0000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512661583754319364096.0000\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512575593148934258688.0000\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512522675853312655360.0000\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1512543364264100888576.0000\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512685086914874703872.0000\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512594311234885517312.0000\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512589807635258146816.0000\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512598955572001243136.0000\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512673264965852856320.0000\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512527460927916736512.0000\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512728715536264855552.0000\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512694375589106155520.0000\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1512531120102613975040.0000\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512662850391714562048.0000\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512705353113197871104.0000\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512436825985415905280.0000\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1512850171988715503616.0000\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512654265404924887040.0000\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512588118785397882880.0000\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512844120276716224512.0000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512566726687167873024.0000\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512521268478429102080.0000\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1512648635905390673920.0000\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512750389109471576064.0000\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512640613868554420224.0000\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512535201489776279552.0000\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512814143191696539648.0000\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512510572429314097152.0000\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512722100874312155136.0000\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512488054431177244672.0000\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512584318873212289024.0000\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1512661865229296074752.0000\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512545193851449507840.0000\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512478765756945793024.0000\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512706479013104713728.0000\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512940666193727979520.0000\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512574889461492482048.0000\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512586570673025974272.0000\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 1512578126423724654592.0000\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 26036824572851912704.0000\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 973121833777288445952.0000\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2314193105135303917568.0000\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2810238567654818316288.0000\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2929871625206334750720.0000\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2955880194529376075776.0000\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2960963632608770523136.0000\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962098821189844598784.0000\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962440531811571335168.0000\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962265735851034017792.0000\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962883291949937197056.0000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962258980451592962048.0000\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962477123558543720448.0000\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962440813286548045824.0000\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962318090196702199808.0000\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2962957038393835388928.0000\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962764509509765300224.0000\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962289661224054423552.0000\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962398029090088026112.0000\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2962297542523402321920.0000\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962163841909464760320.0000\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962150049635605938176.0000\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962649386244290641920.0000\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962173412058672922624.0000\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962807575181202030592.0000\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962501330406540836864.0000\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962698925840191717376.0000\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2962480782733240958976.0000\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962511744980679131136.0000\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962777738833670701056.0000\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962580143400019820544.0000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962553403277232308224.0000\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962400843839855132672.0000\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962556218026999414784.0000\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962207752005831622656.0000\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962355807843581427712.0000\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2962469805209149243392.0000\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962990815391040667648.0000\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962556499501976125440.0000\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962345393269443133440.0000\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962539048053420064768.0000\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962760850335068061696.0000\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962510056130818867200.0000\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962899898973563125760.0000\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962656423118708408320.0000\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962532855603932430336.0000\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 2962564943751277445120.0000\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962403940064598949888.0000\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962381985016415518720.0000\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 2962840507753477177344.0000\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 13ms/step - loss: 16850520371504349184.0000\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 1653821706787178414080.0000\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 3780419480506689650688.0000\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4525995465367456055296.0000\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4706885359150559133696.0000\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4745393950714343981056.0000\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4753926020208397385728.0000\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4755769681305852182528.0000\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4756020756985078087680.0000\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756060163481817579520.0000\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756520656543716212736.0000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4755963336089829113856.0000\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755965024939689377792.0000\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755972906239037276160.0000\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755948699391040159744.0000\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756186264271383953408.0000\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4755497776478349688832.0000\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756647320283236007936.0000\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756170501672688156672.0000\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755841738899890110464.0000\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4756606787886589673472.0000\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756203152769986592768.0000\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756320809310251646976.0000\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756207093419660541952.0000\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755882834246489866240.0000\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755735341358693482496.0000\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755771370155712446464.0000\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756320809310251646976.0000\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4755815280252079308800.0000\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756627617034866262016.0000\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755443733282821242880.0000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755971217389177012224.0000\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755843990699703795712.0000\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755956017740434636800.0000\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756100695878463913984.0000\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755699875511627939840.0000\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4756382170855174569984.0000\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755738156108460589056.0000\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755933499742297784320.0000\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4755774184905479553024.0000\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756223418968309760000.0000\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756557811240642019328.0000\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755775873755339816960.0000\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756109703077718654976.0000\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 4756031453034193092608.0000\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756157553823759466496.0000\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4757328489726875795456.0000\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756238618617052135424.0000\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4756133909925715771392.0000\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 4755842864799796953088.0000\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 19405227942060490752.0000\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 331098029942326165504.0000\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 660811202999235903488.0000\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 770048123023432613888.0000\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 795790768862226546688.0000\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 801322244735800180736.0000\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802493462113893220352.0000\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802708086783635095552.0000\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802744608161863303168.0000\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802876408819708067840.0000\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802851850127990063104.0000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 802760089285582389248.0000\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802737641656189714432.0000\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802728212244469907456.0000\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802741441568375308288.0000\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802694857459729694720.0000\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802695701884659826688.0000\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802737782393678069760.0000\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802739611981026689024.0000\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 802750800611350937600.0000\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802797806932461617152.0000\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802907722910867128320.0000\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802761707766698475520.0000\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802801888319623921664.0000\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802952336694675767296.0000\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802956558819326427136.0000\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802764311410233049088.0000\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 802717445826610724864.0000\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802747211805397876736.0000\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802645317863828619264.0000\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802759315229396434944.0000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802671565405406887936.0000\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802708931208565227520.0000\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802926300259330031616.0000\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802759667073117323264.0000\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 802888582612450803712.0000\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802753052411164622848.0000\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802763044772837851136.0000\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802648765932293324800.0000\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802876268082219712512.0000\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802783099864928485376.0000\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802765718785116602368.0000\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802688383535265349632.0000\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 802838480066596306944.0000\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802785562770974703616.0000\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802818002762040606720.0000\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802869934895243722752.0000\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802831724667155251200.0000\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 802705553508844699648.0000\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 802814132481110835200.0000\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 1s 14ms/step - loss: 10687973202098913280.0000\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 190526699784187674624.0000\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 389665478324558757888.0000\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 456792615932934488064.0000\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 472785742634657251328.0000\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 476254042113313865728.0000\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 476875890705611882496.0000\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 477017331881408987136.0000\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477040236907638816768.0000\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477003398870061809664.0000\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477089706134795714560.0000\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477025353918245240832.0000\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477056245796939235328.0000\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477090445006609580032.0000\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 476989852886807609344.0000\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 477137873540185325568.0000\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477047344150800760832.0000\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477047801547637915648.0000\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477049384844381913088.0000\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477082985919726747648.0000\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 477005861776108027904.0000\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 477013918997316370432.0000\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 477087454334982029312.0000\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477085202535168344064.0000\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 477079432298145775616.0000\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 477089741319167803392.0000\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 0s 12ms/step - loss: 477012230147456106496.0000\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477134249549860175872.0000\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477169363553204830208.0000\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 11ms/step - loss: 477069475120844636160.0000\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477055858768846258176.0000\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477061241977775849472.0000\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477014622684758147072.0000\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 10ms/step - loss: 477100050340189831168.0000\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 9ms/step - loss: 477079467482517864448.0000\n",
      "Epoch 36/50\n",
      " 1/16 [>.............................] - ETA: 0s - loss: 463338281331968704512.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m initial_params \u001b[39m=\u001b[39m [\u001b[39m0.1\u001b[39m, \u001b[39m0.05\u001b[39m, \u001b[39m3.0\u001b[39m, \u001b[39m1e-5\u001b[39m, \u001b[39m0.001\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[39m# Otimização dos hiperparâmetros\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m result \u001b[39m=\u001b[39m minimize(cost_function, initial_params, method\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m'\u001b[39;49m, bounds\u001b[39m=\u001b[39;49m[(\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), (\u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m), (\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), (\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), (\u001b[39m0\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m)])\n\u001b[1;32m     16\u001b[0m \u001b[39m# Exibir os resultados otimizados\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimal_params \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mx\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[1;32m    711\u001b[0m                            callback\u001b[39m=\u001b[39;49mcallback, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\n\u001b[1;32m    712\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_lbfgsb_py.py:361\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    355\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    357\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[1;32m    362\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    363\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:286\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m    285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[0;32m--> 286\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad()\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_grad\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated:\n\u001b[0;32m--> 256\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_grad_impl()\n\u001b[1;32m    257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_fun()\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg \u001b[39m=\u001b[39m approx_derivative(fun_wrapped, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx, f0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf,\n\u001b[1;32m    174\u001b[0m                            \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfinite_diff_options)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m sparsity \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m _dense_difference(fun_wrapped, x0, f0, h,\n\u001b[1;32m    506\u001b[0m                              use_one_sided, method)\n\u001b[1;32m    507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m issparse(sparsity) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(sparsity) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[39m=\u001b[39m x[i] \u001b[39m-\u001b[39m x0[i]  \u001b[39m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[39m=\u001b[39m fun(x) \u001b[39m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[39melif\u001b[39;00m method \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m3-point\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[39m=\u001b[39m x0 \u001b[39m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39matleast_1d(fun(x, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    457\u001b[0m     \u001b[39mif\u001b[39;00m f\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`fun` return value has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[39m\"\u001b[39m\u001b[39mmore than 1 dimension.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[27], line 4\u001b[0m, in \u001b[0;36mcost_function\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcost_function\u001b[39m(params):\n\u001b[1;32m      3\u001b[0m     sparsity, rho, beta, epsilon, l1_lambda \u001b[39m=\u001b[39m params\n\u001b[0;32m----> 4\u001b[0m     cost \u001b[39m=\u001b[39m autoencoderEsparso(sparsity, rho, beta, epsilon, l1_lambda)\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m cost\n",
      "Cell \u001b[0;32mIn[26], line 47\u001b[0m, in \u001b[0;36mautoencoderEsparso\u001b[0;34m(sparsity, rho, beta, epsilon, l1_lambda)\u001b[0m\n\u001b[1;32m     43\u001b[0m autoencoder\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[39m# Treine o modelo com seus dados\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Substitua X_train pelos seus dados de treinamento\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m autoencoder\u001b[39m.\u001b[39;49mfit(X_train, X_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m64\u001b[39;49m) \u001b[39m#Callback\u001b[39;00m\n\u001b[1;32m     48\u001b[0m cost \u001b[39m=\u001b[39m autoencoder\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m cost\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1775\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1777\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1781\u001b[0m ):\n\u001b[1;32m   1782\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1783\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1784\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1785\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    828\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 831\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    833\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    834\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    865\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    866\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 867\u001b[0m   \u001b[39mreturn\u001b[39;00m tracing_compilation\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    868\u001b[0m       args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_config\n\u001b[1;32m    869\u001b[0m   )\n\u001b[1;32m    870\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39;49m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1261\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1262\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1263\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1264\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mflat_call(args)\n\u001b[1;32m   1265\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1266\u001b[0m     args,\n\u001b[1;32m   1267\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1268\u001b[0m     executing_eagerly)\n\u001b[1;32m   1269\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[0m, in \u001b[0;36mAtomicFunction.flat_call\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mflat_call\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    218\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    251\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 252\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[1;32m    253\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m    254\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[1;32m    255\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    259\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    260\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    261\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    262\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1477\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1478\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1479\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m   1480\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1481\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[1;32m   1482\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[1;32m   1483\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m   1484\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1485\u001b[0m   )\n\u001b[1;32m   1486\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1487\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1488\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1489\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1493\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1494\u001b[0m   )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m   \u001b[39m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m   inputs \u001b[39m=\u001b[39m [\n\u001b[1;32m     55\u001b[0m       tensor_conversion_registry\u001b[39m.\u001b[39mconvert(t)\n\u001b[1;32m     56\u001b[0m       \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, core_types\u001b[39m.\u001b[39mTensor)\n\u001b[1;32m     57\u001b[0m       \u001b[39melse\u001b[39;00m t\n\u001b[1;32m     58\u001b[0m       \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m inputs\n\u001b[1;32m     59\u001b[0m   ]\n\u001b[0;32m---> 60\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     61\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     62\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     63\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Função de custo para otimização\n",
    "def cost_function(params):\n",
    "    sparsity, rho, beta, epsilon, l1_lambda = params\n",
    "    cost = autoencoderEsparso(sparsity, rho, beta, epsilon, l1_lambda)\n",
    "    return cost\n",
    "\n",
    "# Seus dados de treinamento\n",
    "# Substitua X_train pelos seus dados de treinamento\n",
    "\n",
    "# Valores iniciais dos hiperparâmetros\n",
    "initial_params = [0.1, 0.05, 3.0, 1e-5, 0.001]\n",
    "\n",
    "# Otimização dos hiperparâmetros\n",
    "result = minimize(cost_function, initial_params, method='L-BFGS-B', bounds=[(0, 1), (0, 1), (0, None), (0, None), (0, None)])\n",
    "\n",
    "# Exibir os resultados otimizados\n",
    "optimal_params = result.x\n",
    "optimal_cost = result.fun\n",
    "\n",
    "print(\"Hiperparâmetros otimizados:\", optimal_params)\n",
    "print(\"Custo otimizado:\", optimal_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = autoencoder.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (5830,) and (3000, 5830)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[1;32m      7\u001b[0m first_row_list \u001b[39m=\u001b[39m X_test\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m----> 9\u001b[0m plt\u001b[39m.\u001b[39;49mplot(x, prediction)\n\u001b[1;32m     10\u001b[0m plt\u001b[39m.\u001b[39mplot(x,first_row_list)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (5830,) and (3000, 5830)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(5830):\n",
    "    x.append(i)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "first_row_list = X_test.iloc[0].tolist()\n",
    "\n",
    "plt.plot(x, prediction)\n",
    "plt.plot(x,first_row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
