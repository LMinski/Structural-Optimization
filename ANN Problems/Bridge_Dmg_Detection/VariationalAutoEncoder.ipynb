{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras import layers, Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Supondo que você tenha as bibliotecas necessárias importadas e os dados carregados como mencionado\n",
    "\n",
    "dataTT = loadmat(f\"Data04-08_TT_UltVag_Cut.mat\")\n",
    "dataVG = loadmat(f\"Data04-08_TF_UltVag_Cut.mat\")\n",
    "\n",
    "dataBaseline = dataTT['Baseline']\n",
    "dataCincoP = dataTT['CincoP']\n",
    "dataDez = dataTT['DezP']\n",
    "dataVinte = dataTT['VinteP']\n",
    "\n",
    "dadosRigidez = pd.DataFrame()\n",
    "\n",
    "# Adicionar dataBaseline com coluna y_Baseline\n",
    "dataBaseline_df = pd.DataFrame(dataBaseline)\n",
    "baseline_test_df = dataBaseline_df.sample(n=200, random_state=42)\n",
    "dataBaseline_df = dataBaseline_df.drop(baseline_test_df.index)\n",
    "dataBaseline_df.reset_index(drop=True, inplace=True)  # Redefinir índices\n",
    "dadosRigidez = pd.concat([dadosRigidez, dataBaseline_df], ignore_index=True)\n",
    "\n",
    "# Adicionar dataCincoP com coluna y_CincoP\n",
    "dataCincoP_df = pd.DataFrame(dataCincoP)\n",
    "cinco_test_df = dataCincoP_df.sample(n=200, random_state=42)\n",
    "dataCincoP_df = dataCincoP_df.drop(cinco_test_df.index)\n",
    "dataCincoP_df.reset_index(drop=True, inplace=True)  # Redefinir índices\n",
    "dadosRigidez = pd.concat([dadosRigidez, dataCincoP_df], ignore_index=True)\n",
    "\n",
    "# Adicionar dataDez com coluna y_DezP\n",
    "dataDez_df = pd.DataFrame(dataDez)\n",
    "dez_test_df =  dataDez_df.sample(n=200, random_state=42)\n",
    "dataDez_df = dataDez_df.drop(dez_test_df.index)\n",
    "dataDez_df.reset_index(drop=True, inplace=True)  # Redefinir índices\n",
    "dadosRigidez = pd.concat([dadosRigidez, dataDez_df], ignore_index=True)\n",
    "\n",
    "# Adicionar dataVinte com coluna y_VinteP\n",
    "dataVinte_df = pd.DataFrame(dataVinte)\n",
    "vinte_test_df =  dataVinte_df.sample(n=200, random_state=42)\n",
    "dataVinte_df = dataVinte_df.drop(vinte_test_df.index)\n",
    "dataVinte_df.reset_index(drop=True, inplace=True)  # Redefinir índices\n",
    "dadosRigidez = pd.concat([dadosRigidez, dataVinte_df], ignore_index=True)\n",
    "\n",
    "X_train = dadosRigidez\n",
    "X_test = pd.concat([baseline_test_df, cinco_test_df, dez_test_df, vinte_test_df], ignore_index=True)\n",
    "\n",
    "num_terms = X_train.columns.shape[0]\n",
    "\n",
    "# Reformulando os dados para se adequarem à entrada da rede\n",
    "X_train = X_train.values.reshape(-1, num_terms, 1)\n",
    "X_test = X_test.values.reshape(-1, num_terms, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 1s 7ms/step - loss: 0.0053 - val_loss: 0.0014\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0013 - val_loss: 0.0011\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 8.0358e-04 - val_loss: 6.6109e-04\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 6.2556e-04 - val_loss: 5.5584e-04\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 4.9237e-04 - val_loss: 5.8206e-04\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 5.2158e-04 - val_loss: 4.9041e-04\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 5.5223e-04 - val_loss: 6.1744e-04\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 7.3448e-04 - val_loss: 8.9153e-04\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0030 - val_loss: 0.0039\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0032 - val_loss: 8.0491e-04\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0057 - val_loss: 0.0130\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0026 - val_loss: 6.2816e-04\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 6.0744e-04 - val_loss: 5.6208e-04\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 6.7798e-04 - val_loss: 4.9556e-04\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 4.6699e-04 - val_loss: 3.3339e-04\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3.9042e-04 - val_loss: 3.6923e-04\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 4.4244e-04 - val_loss: 3.6527e-04\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 3.6326e-04 - val_loss: 4.9874e-04\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 5.7655e-04 - val_loss: 9.9563e-04\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 6.2269e-04\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 6.6464e-04 - val_loss: 0.0013\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0324 - val_loss: 0.0011\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 4.4102e-04 - val_loss: 4.5494e-04\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.2076e-04 - val_loss: 3.1575e-04\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 3.3857e-04 - val_loss: 5.3304e-04\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0017 - val_loss: 0.0150\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 1s 6ms/step - loss: 0.0123 - val_loss: 0.0030\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 1s 5ms/step - loss: 0.0035 - val_loss: 0.0010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x313059d90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_encoder = Sequential([\n",
    "            layers.Input(shape=(num_terms,)),\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(16, activation='relu')\n",
    "            ])\n",
    "\n",
    "stacked_decoder = Sequential([\n",
    "            layers.Dense(16, activation='relu'),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dense(64, activation='relu'),    \n",
    "            layers.Dense(num_terms, activation='linear')\n",
    "            ])\n",
    "\n",
    "autoencoder = Sequential([stacked_encoder, stacked_decoder])\n",
    "\n",
    "autoencoder.compile(optimizer=tf.keras.optimizers.legacy.Adam(), loss='mse')\n",
    "\n",
    "autoencoder.fit(\n",
    "            X_train, X_train,\n",
    "            epochs=30,\n",
    "            validation_data=(X_test, X_test),\n",
    "            verbose = 1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Extract the minimum and maximum values of the latent space. \n",
    "latent = pd.DataFrame(stacked_encoder.predict(X_train))\n",
    "mins = latent.min(axis = 0).values\n",
    "maxs = latent.max(axis = 0).values\n",
    "mean = latent.mean(axis = 0).values\n",
    "stddev = latent.std(axis = 0).values\n",
    "#print(latent.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampling(keras.layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        mean, log_var = inputs\n",
    "        return K.random_normal(tf.shape(log_var))*K.exp(log_var/2)+mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
